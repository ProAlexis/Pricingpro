# ðŸ”§ Diagnostic du Workflow de Scraping

## âŒ ProblÃ¨me Actuel

Le workflow GitHub Actions "Scrape Market Rates" Ã©choue systÃ©matiquement.

## ðŸ” Causes Probables

### 1. **Secrets GitHub Manquants** (PRIORITAIRE)

VÃ©rifiez que les secrets suivants sont configurÃ©s dans GitHub :

**Aller Ã  :** `Settings` â†’ `Secrets and variables` â†’ `Actions` â†’ `Repository secrets`

**Secrets requis :**
- `SUPABASE_URL` : URL de votre projet Supabase
- `SUPABASE_SERVICE_ROLE_KEY` : **ClÃ© admin (service_role)** - NÃ©cessaire pour bypass RLS et faire des INSERT/DELETE en masse

### 2. **Permissions Supabase**

Le scraper doit pouvoir :
- `DELETE` sur la table `market_rates` (ligne 26-29 de master-scraper.js)
- `INSERT` massif sur `market_rates` (batches de 100)
- `INSERT` massif sur `market_rates_history`

**VÃ©rifier les Row Level Security (RLS) policies :**
```sql
-- Dans Supabase SQL Editor
SELECT tablename, policyname, cmd, qual
FROM pg_policies
WHERE tablename IN ('market_rates', 'market_rates_history');
```

**Si RLS est activÃ©, ajouter :**
```sql
-- Permettre INSERT/DELETE avec la clÃ© de service
CREATE POLICY "Allow service role full access on market_rates"
ON market_rates
FOR ALL
TO service_role
USING (true)
WITH CHECK (true);

CREATE POLICY "Allow service role full access on market_rates_history"
ON market_rates_history
FOR ALL
TO service_role
USING (true)
WITH CHECK (true);
```

### 3. **Limites de Rate Nominatim**

Le scraper utilise l'API Nominatim (OpenStreetMap) pour gÃ©ocoder les villes.

**Limite :** 1 requÃªte/seconde (respectÃ©e dans le code ligne 194)

Si trop de villes inconnues â†’ timeout possible.

### 4. **Timeout GitHub Actions**

Le scraper peut prendre 2-5 minutes. Si > 6 minutes â†’ timeout.

**Optimisation actuelle :**
- Scraping parallÃ¨le (Promise.all ligne 41-48)
- Insertion par batch de 100 (ligne 86-98)

## âœ… Tests de Diagnostic

### Test Local (RecommandÃ©)

```bash
# 1. CrÃ©er un fichier .env local avec vos clÃ©s
cat > .env << EOF
SUPABASE_URL=votre_url_supabase
SUPABASE_SERVICE_ROLE_KEY=votre_cle_service_role
EOF

# 2. Installer les dÃ©pendances
npm install

# 3. Tester le scraper
node run-scraper.js

# âœ… Si succÃ¨s : vous devriez voir ~3500+ tarifs insÃ©rÃ©s
# âŒ Si Ã©chec : noter le message d'erreur exact
```

### Test GitHub Actions (Manuel)

1. Aller dans l'onglet **Actions** du repo
2. SÃ©lectionner "Scrape Market Rates"
3. Cliquer **Run workflow** â†’ **Run workflow**
4. Attendre 2-5 minutes
5. Si Ã©chec â†’ TÃ©lÃ©charger les logs (artifact "scraper-logs")

## ðŸ› ï¸ Corrections Possibles

### Si "Supabase credentials missing"
â†’ Ajouter les secrets dans GitHub Settings

### Si "Permission denied" sur Supabase
â†’ DÃ©sactiver RLS ou ajouter policies (voir ci-dessus)

### Si "Rate limit exceeded" (Nominatim)
â†’ Le code respecte dÃ©jÃ  la limite. Erreur rare.

### Si "Timeout"
â†’ Augmenter le timeout dans le workflow :

```yaml
# .github/workflows/scrape-rates.yml
jobs:
  scrape:
    runs-on: ubuntu-latest
    timeout-minutes: 15  # â† Ajouter cette ligne
```

## ðŸ“Š DonnÃ©es Attendues

AprÃ¨s un scraping rÃ©ussi :

| Source | Tarifs |
|--------|--------|
| Public Data (Malt, Free-Work, Stack Overflow) | ~500-800 |
| Enhanced Malt | ~1500-2000 |
| Upwork | ~200-400 |
| Glassdoor | ~300-500 |
| **TOTAL** | **~2500-3700** |

## ðŸ”” Monitoring

AprÃ¨s correction, configurer des alertes :

1. **GitHub Actions Notifications**
   - Settings â†’ Notifications â†’ Actions
   - Cocher "Email notifications for failed workflows"

2. **Alternative : Slack/Discord webhook**
   ```yaml
   - name: Notify on failure
     if: failure()
     run: |
       curl -X POST ${{ secrets.SLACK_WEBHOOK }} \
         -d '{"text":"âš ï¸ Scraping failed!"}'
   ```

## ðŸ“ Prochaines Ã‰tapes

1. âœ… VÃ©rifier les secrets GitHub
2. âœ… Tester localement avec `node run-scraper.js`
3. âœ… VÃ©rifier les permissions Supabase
4. âœ… DÃ©clencher manuellement le workflow
5. âœ… Analyser les logs en cas d'Ã©chec
